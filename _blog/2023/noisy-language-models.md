---
title: "Noisy language models"
slug: "noisy-language-models"
date: "2023-01-16"
toc: false
---

My initial reaction reading [this post titled "The End of Organizing"](https://every.to/superorganizers/the-end-of-organizing) was one of enthusiasm. "Yes indeed, I want an automated tool to organize my notes for me. That'd be amazing!"

It did not take long for me to become gravely depressed with the current state of affairs. LLMs (Large Language Models) are hailed as the answer to everything, and I believe this specifically is a problem which can be easily, if not better, be solved through the use of more traditional linguistic tools such as [NLTK](https://www.nltk.org/book/ch05.html). 

The article mentioned above starts with an introduction about the problem. Personal knowledge bases, zettlekasten and what not are difficult to organize, and to keep organized. This in itself is something I have given upon for a long time ago. I have to agree with the author; that's a big problem which should be tackled sometime. Furthermore I personally would love if someone were to come up with a great solution for this problem.

But then they continued about LLMs. How cool would it be to be in a conversation with your past self? Certainly, that'd be amazing, but what for? Questions started to arise. Basal ones, such as whether or not I'd be willing to connect my notes to an online service for analysis. Also more complicated ones, such as the potential impact a conversation with your former self would have on the quality of your notes.

This where I think a main weakness of these LLMs in personal note taking lies. It's a great good to be able to change every now and then as person. Taking on new identities as you deem fit, shedding old ones as you deem necessary. It's this process I fear will be further constrained when in a continuous conversation with your former self. How much would it inhibit your personal development? How greatly would it impair ones flexibility to change?

This all still assumes that the output quality of these LLMs is comparable to human beings. We know that those LLMs are very well trained in the art of bullshitting, and can present incorrect information in a convincing manner. Is that really what we want in our notetaking applications?

And even if it were able to present our own thoughts coherently back to us, I still bet it's easier to recall context from our own notes than it is to recall this context from our own notes that had been mangled by a LLM. The cognitive effort is simply lower because there are more triggers contained within our own work. This would especially be the case if a LLM came up with relationships we ourselves had never seen before. Understanding these as well would take a lot of cognitive effort, if we succeed at all.

At the same time I realize I am being highly skeptical of the power of LLMs. Even though the Turing test has definitively been cracked, I still think too much potential power is attributed to these AI systems. Though I certainly see a future for these systems in mainstream applications (think about search), I am also convinced there will be places where the impact of LLMs is greatly discouraged. Think in this context about sanctuaries explicitly designed for original thought, analysis, reflection and more. Places where we can be free from interference between the past, present and future. A place where we do not have to put up with giving meaning to things which deserve none. A place where we can reduce the noise of the outside world, of machines, pieces of software even, and tune into ourselves.
